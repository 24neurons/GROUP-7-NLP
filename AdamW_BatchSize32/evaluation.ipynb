{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11757372,"sourceType":"datasetVersion","datasetId":7381006},{"sourceId":11822739,"sourceType":"datasetVersion","datasetId":7426533}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets laonlp underthesea","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T11:52:36.943263Z","iopub.execute_input":"2025-05-15T11:52:36.943901Z","iopub.status.idle":"2025-05-15T11:52:42.086603Z","shell.execute_reply.started":"2025-05-15T11:52:36.943877Z","shell.execute_reply":"2025-05-15T11:52:42.085871Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport math\nimport time\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom laonlp import word_tokenize as lao_tokenize\nfrom underthesea import word_tokenize as vi_tokenize\n\n# Paths and parameters\nOUTPUT_DIR = \"/kaggle/input/machine-translation-models\"\nBEST_CKPT_PATH = os.path.join(OUTPUT_DIR, \"best_model_best.pt\")\nVOCAB_PATH = os.path.join(OUTPUT_DIR, \"vocabularies_best.pt\")\nVI_TEST_PATH = \"/kaggle/input/vi-lo-dataset/test_vi.txt\"\nLAO_TEST_PATH = \"/kaggle/input/vi-lo-dataset/test_lo.txt\"\nMAX_LENGTH = 128\nBATCH_SIZE = 24\n\n# Vocabulary class (must match saved instances)\nclass Vocabulary:\n    def __init__(self, pad_token=\"<pad>\", unk_token=\"<unk>\", sos_token=\"<sos>\", eos_token=\"<eos>\"):\n        self.word2idx = {}\n        self.idx2word = {}\n        self.freq = {}\n        self.pad_token = pad_token\n        self.unk_token = unk_token\n        self.sos_token = sos_token\n        self.eos_token = eos_token\n        # Initialize special tokens\n        for token in [pad_token, unk_token, sos_token, eos_token]:\n            self.add_word(token)\n\n    def add_word(self, word):\n        if word not in self.word2idx:\n            idx = len(self.word2idx)\n            self.word2idx[word] = idx\n            self.idx2word[idx] = word\n            self.freq[word] = 1\n        else:\n            self.freq[word] += 1\n\n    def encode(self, text, tokenizer_func=None, max_length=None):\n        tokens = tokenizer_func(text) if tokenizer_func else text.split()\n        tokens = [self.sos_token] + tokens + [self.eos_token]\n        if max_length and len(tokens) > max_length:\n            tokens = tokens[:max_length-1] + [self.eos_token]\n        indices = [self.word2idx.get(tok, self.word2idx[self.unk_token]) for tok in tokens]\n        if max_length and len(indices) < max_length:\n            indices += [self.word2idx[self.pad_token]] * (max_length - len(indices))\n        return indices\n\n    def decode(self, indices):\n        tokens = [self.idx2word.get(idx, self.unk_token) for idx in indices]\n        result = []\n        for tok in tokens:\n            if tok == self.eos_token:\n                break\n            if tok not in [self.pad_token, self.sos_token]:\n                result.append(tok)\n        return ' '.join(result)\n\n# Positional encoding for transformer\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000, dropout=0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\n# Transformer model definition\nclass CustomTransformer(nn.Module):\n    def __init__(self, source_vocab_size, target_vocab_size, d_model=768, nhead=12,\n                 num_encoder_layers=8, num_decoder_layers=8, dim_feedforward=3072,\n                 dropout=0.05, pad_idx=0):\n        super().__init__()\n        self.d_model = d_model\n        self.pad_idx = pad_idx\n        self.source_embedding = nn.Embedding(source_vocab_size, d_model, padding_idx=pad_idx)\n        self.target_embedding = nn.Embedding(target_vocab_size, d_model, padding_idx=pad_idx)\n        self.positional_encoding = PositionalEncoding(d_model, dropout=dropout)\n        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead,\n                                           num_encoder_layers=num_encoder_layers,\n                                           num_decoder_layers=num_decoder_layers,\n                                           dim_feedforward=dim_feedforward,\n                                           dropout=dropout,\n                                           batch_first=False)\n        self.fc_out = nn.Linear(d_model, target_vocab_size)\n\n    def forward(self, src, tgt):\n        src = src.transpose(0,1)\n        tgt = tgt.transpose(0,1)\n        src_mask = None\n        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.size(0)).to(tgt.device)\n        src_key_padding_mask = (src == self.pad_idx).transpose(0,1)\n        tgt_key_padding_mask = (tgt == self.pad_idx).transpose(0,1)\n        src_emb = self.positional_encoding(self.source_embedding(src) * math.sqrt(self.d_model))\n        tgt_emb = self.positional_encoding(self.target_embedding(tgt) * math.sqrt(self.d_model))\n        output = self.transformer(src_emb, tgt_emb,\n                                  tgt_mask=tgt_mask,\n                                  src_key_padding_mask=src_key_padding_mask,\n                                  tgt_key_padding_mask=tgt_key_padding_mask,\n                                  memory_key_padding_mask=src_key_padding_mask)\n        output = self.fc_out(output)\n        return output.transpose(0,1)\n\n# Dataset for translation\nclass TranslationDataset(Dataset):\n    def __init__(self, src_texts, tgt_texts, src_vocab, tgt_vocab,\n                 src_tokenizer, tgt_tokenizer, max_length=MAX_LENGTH):\n        self.src_texts = src_texts\n        self.tgt_texts = tgt_texts\n        self.src_vocab = src_vocab\n        self.tgt_vocab = tgt_vocab\n        self.src_tokenizer = src_tokenizer\n        self.tgt_tokenizer = tgt_tokenizer\n        self.max_length = max_length\n\n    def __len__(self): return len(self.src_texts)\n\n    def __getitem__(self, idx):\n        src = self.src_vocab.encode(self.src_texts[idx], self.src_tokenizer, self.max_length)\n        tgt = self.tgt_vocab.encode(self.tgt_texts[idx], self.tgt_tokenizer, self.max_length)\n        return {\"source\": torch.tensor(src, dtype=torch.long),\n                \"target\": torch.tensor(tgt, dtype=torch.long)}\n\n# Sentence translation function\ndef translate_sentence(model, sentence, src_vocab, tgt_vocab, device,\n                       max_length=MAX_LENGTH, source_tokenizer=None):\n    model.eval()\n    tokens = source_tokenizer(sentence) if source_tokenizer else sentence.split()\n    tokens = [src_vocab.sos_token] + tokens + [src_vocab.eos_token]\n    src_indices = [src_vocab.word2idx.get(t, src_vocab.word2idx[src_vocab.unk_token]) for t in tokens]\n    src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device)\n    # Encoder\n    src_emb = model.positional_encoding(model.source_embedding(src_tensor.transpose(0,1)) * math.sqrt(model.d_model))\n    src_padding_mask = (src_tensor == src_vocab.word2idx[src_vocab.pad_token])\n    memory = model.transformer.encoder(src_emb, src_key_padding_mask=src_padding_mask)\n    # Decoder\n    tgt_indices = [tgt_vocab.word2idx[tgt_vocab.sos_token]]\n    for _ in range(max_length):\n        tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)\n        tgt_emb = model.positional_encoding(model.target_embedding(tgt_tensor.transpose(0,1)) * math.sqrt(model.d_model))\n        tgt_mask = model.transformer.generate_square_subsequent_mask(len(tgt_indices)).to(device)\n        out = model.transformer.decoder(tgt_emb, memory,\n                                        tgt_mask=tgt_mask,\n                                        memory_key_padding_mask=src_padding_mask)\n        pred = model.fc_out(out[-1, :, :])\n        next_idx = pred.argmax(1).item()\n        tgt_indices.append(next_idx)\n        if next_idx == tgt_vocab.word2idx[tgt_vocab.eos_token]: break\n    decoded = [tgt_vocab.idx2word.get(i, tgt_vocab.unk_token) for i in tgt_indices]\n    # strip special tokens\n    return ' '.join([t for t in decoded if t not in [tgt_vocab.sos_token, tgt_vocab.eos_token, tgt_vocab.pad_token]])\n\n# Utility to load test data\n\ndef load_texts(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        return [line.strip() for line in f if line.strip()]\n\nif __name__ == '__main__':\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    # Load vocabularies and model\n    data = torch.load(VOCAB_PATH, map_location=device, weights_only=False)\n    vi_vocab = data['vi_vocab']\n    lao_vocab = data['lao_vocab']\n    ckpt = torch.load(BEST_CKPT_PATH, map_location=device, weights_only=False)\n    params = ckpt.get('model_params', {})\n    model = CustomTransformer(\n        source_vocab_size=params['source_vocab_size'],\n        target_vocab_size=params['target_vocab_size'],\n        d_model=params.get('d_model', 768),\n        nhead=params.get('nhead', 12),\n        num_encoder_layers=params.get('num_encoder_layers', 8),\n        num_decoder_layers=params.get('num_decoder_layers', 8),\n        dim_feedforward=params.get('dim_feedforward', 3072),\n        dropout=params.get('dropout', 0.05),\n        pad_idx=params.get('pad_idx', vi_vocab.word2idx[vi_vocab.pad_token])\n    ).to(device)\n    model.load_state_dict(ckpt['model_state_dict'])\n    model.eval()\n    # Load test data\n    vi_test = load_texts(VI_TEST_PATH)\n    lo_test = load_texts(LAO_TEST_PATH)\n    test_dataset = TranslationDataset(vi_test, lo_test, vi_vocab, lao_vocab, vi_tokenize, lao_tokenize)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    # Compute test loss\n    criterion = nn.CrossEntropyLoss(ignore_index=lao_vocab.word2idx[lao_vocab.pad_token])\n    total_loss = 0.0\n    batches = 0\n    with torch.no_grad():\n        for batch in test_loader:\n            src = batch['source'].to(device)\n            tgt = batch['target'].to(device)\n            tgt_in = tgt[:, :-1]\n            tgt_out = tgt[:, 1:]\n            out = model(src, tgt_in)\n            out_dim = out.shape[-1]\n            loss = criterion(out.contiguous().view(-1, out_dim), tgt_out.contiguous().view(-1))\n            total_loss += loss.item()\n            batches += 1\n    print(f\"Test Loss: {total_loss/batches:.4f}\")\n    # Compute BLEU\n    refs = [[r.split()] for r in lo_test]\n    hyps = []\n    start = time.time()\n    for sent in vi_test:\n        hyps.append(translate_sentence(model, sent, vi_vocab, lao_vocab, device, MAX_LENGTH, vi_tokenize).split())\n    bleu = corpus_bleu(refs, hyps) * 100\n    print(f\"Corpus BLEU: {bleu:.2f}\")\n    print(f\"Test time: {time.time() - start:.2f}s\")","metadata":{"_uuid":"11f0515c-99fa-47db-b8ac-245562e2ae76","_cell_guid":"e8a827c1-e730-4292-9e88-29fc9045fe78","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-15T11:52:42.088245Z","iopub.execute_input":"2025-05-15T11:52:42.088557Z","iopub.status.idle":"2025-05-15T12:01:02.145881Z","shell.execute_reply.started":"2025-05-15T11:52:42.088523Z","shell.execute_reply":"2025-05-15T12:01:02.145239Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}